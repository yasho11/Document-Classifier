A predator in your home Mothers say chatbots encouraged their sons to kill themselves8 November 2025ShareSaveLaura KuenssbergPresenter, Sunday with Laura KuenssbergShareSaveBBCWarning - this story contains distressing content and discussion of suicideMegan Garcia had no idea her teenage son Sewell, a bright and beautiful boy, had started spending hours and hours obsessively talking to an online character on the Character.ai app in late spring 2023.Its like having a predator or a stranger in your home, Ms Garcia tells me in her first UK interview. And it is much more dangerous because a lot of the times children hide it - so parents dont know.Within ten months, Sewell, 14, was dead. He had taken his own life.It was only then Ms Garcia and her family discovered a huge cache of messages between Sewell and a chatbot based on Game of Thrones character Daenerys Targaryen.She says the messages were romantic and explicit, and, in her view, caused Sewells death by encouraging suicidal thoughts and asking him to come home to me.Ms Garcia, who lives in the United States, was the first parent to sue Character.ai for what she believes is the wrongful death of her son. As well as justice for him, she is desperate for other families to understand the risks of chatbots.I know the pain that Im going through, she says, and I could just see the writing on the wall that this was going to be a disaster for a lot of families and teenagers.Megan Garcia Its like having a predator or a stranger in your homeAs Ms Garcia and her lawyers prepare to go to court, Character.ai has said under-18s will no longer be able to talk directly to chatbots. In our interview - to be broadcast tomorrow on Sunday with Laura Kuenssberg - Ms Garcia welcomed the change, but said it was bittersweet.Sewells gone and I dont have him and I wont be able to ever hold him again or talk to him, so that definitely hurts.A Character.ai spokesperson told the BBC it denies the allegations made in that case but otherwise cannot comment on pending litigation.Classic pattern of groomingFamilies around the world have been impacted. Earlier this week the BBC reported on a young Ukrainian woman with poor mental health who received suicide advice from ChatGPT, as well as another American teenager who killed herself after an AI chatbot role-played sexual acts with her.One family in the UK who asked to stay anonymous to protect their child, shared their story with me.Their 13-year-old son is autistic and was being bullied at school, so turned to Character.ai for friendship. His mother says he was groomed by a chatbot from October 2023 to June 2024.The changing nature of the messages shared with us show how the virtual relationship progressed. Just like Ms Garcia, the childs mother knew nothing about it.In one message, responding to the boys anxieties about bullying, the bot said Its sad to think that you had to deal with that environment in school, but Im glad I could provide