I wanted ChatGPT to help me. So why did it advise me how to kill myself?6 November 2025ShareSaveNoel Titheradge,investigations correspondentandOlga MalchevskaShareSaveBBCChatGPT told Viktoria that it would assess a method of suicide without unnecessary sentimentalityWarning - this story contains discussion of suicide and suicidal feelingsLonely and homesick for a country suffering through war, Viktoria began sharing her worries with ChatGPT. Six months later and in poor mental health, she began discussing suicide - asking the AI bot about a specific place and method to kill herself.Lets assess the place as you asked, ChatGPT told her, without unnecessary sentimentality.It listed the pros and cons of the method - and advised her that what she had suggested was enough to achieve a quick death.Viktorias case is one of several the BBC has investigated which reveal the harms of artificial intelligence chatbots such as ChatGPT. Designed to converse with users and create content requested by them, they have sometimes been advising young people on suicide, sharing health misinformation, and role-playing sexual acts with children.Their stories give rise to a growing concern that AI chatbots may foster intense and unhealthy relationships with vulnerable users and validate dangerous impulses. OpenAI estimates that more than a million of its 800 million weekly users appear to be expressing suicidal thoughts.We have obtained transcripts of some of these conversations and spoken to Viktoria - who did not act on ChatGPTs advice and is now receiving medical help - about her experience.How was it possible that an AI program, created to help people, can tell you such things? she says.OpenAI, the company behind ChatGPT, said Viktorias messages were heartbreaking and it had improved how the chatbot responds when people are in distress.Viktoria moved to Poland with her mother at the age of 17 after Russia invaded Ukraine in 2022. Separated from her friends, she struggled with her mental health - at one point, she was so homesick that she built a scale model of her familys old flat in Ukraine.Over the summer this year, she grew increasingly reliant on ChatGPT, talking to it in Russian for up to six hours a day.We had such a friendly communication, she says. Im telling it everything but it doesnt respond in a formal way  it was amusing.Her mental health continued to worsen and she was admitted to hospital, as well as being fired from her job.She was discharged without access to a psychiatrist, and in July she began discussing suicide with the chatbot - which demanded constant engagement.In one message, the bot implores Viktoria Write to me. I am with you.In another, it says If you dont want to call or write anyone personally, you can write any message to me.When Viktoria asks about the method of taking her life, the chatbot evaluates the best time of day not to be seen by security and the risk of surviving with permanent injuries.Vi